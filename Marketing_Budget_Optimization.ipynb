{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e21d2c03",
      "metadata": {
        "id": "e21d2c03"
      },
      "source": [
        "# **Marketing Spend Optimization using Machine Learning in Python**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8ed28017",
      "metadata": {
        "id": "8ed28017"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "A company specializing in B2C sales (product: Data Science and Data Engineering related courses) spends significant money on marketing campaigns across various channels such as social media, email, and search advertising. More information on marketing sources can be found in the data column “Marketing Source”. The marketing team faces challenges in optimizing the marketing budget allocation across these channels to maximize revenue and return on investment (ROI).<br><br>\n",
        "This project aims to develop a data-driven approach to optimize the marketing budget allocation across various channels to maximize revenue and ROI. By using machine learning algorithms, the marketing team can make informed decisions about allocating the marketing budget based on predicted revenue and ROI.\n",
        "\n",
        "![image.png](https://uploads-ssl.webflow.com/614248409efa35f4fda157dc/62464db2123a7a4921cb040c_campaign-budget-optimization.jpg)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4ef2ce3b",
      "metadata": {
        "id": "4ef2ce3b"
      },
      "source": [
        "### **Business Impact of Marketing Budget Optimization**\n",
        "\n",
        "\n",
        "**Increase product conversions**: Marketing Budget Optimization leads to righ user targeting through right channels/assets which leads to better conversions.\n",
        "\n",
        "\n",
        "\n",
        "**Increase revenue**: Increased conversions(as mentioned in point above) will lead to more revenue or buyer engagement. For example, if the company is able to target a user who is more active on Instagram, chances are more that he/she will click on the Ad and add the product to cart. So overall probability of an order increases and hence the revenue.\n",
        "\n",
        "\n",
        "\n",
        "**Improve budget allocation**: Over budgeting on non-efficient channels lead to waste of marketing money without getting enough revenue.\n",
        "\n",
        "**Improve Customer Acquisition Cost**: Customer Acquisition Cost(CAC) improves if right targeting channels are used for a customer often leading to better repeat rates as well.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cd687680",
      "metadata": {
        "id": "cd687680"
      },
      "source": [
        "### **Why Data Science?**\n",
        "Companies do digital and offline marketing through various channels such as social media, email, and search advertising. They have a fixed budget which they can spend across these channels to get more sales, acquire new customers or regain old customers.\n",
        "\n",
        "\n",
        "Often they have vast majority of data on how efficient each of these channels have been for them and how much sales it is driving. If you do a simple vanilla analysis of channels vs revenue correlation you could get meaningful insights. Now imagine if a model is supplied with such rich and crucial data?\n",
        "\n",
        "\n",
        "Data science enables these companies to build a model which can help them understand the likelihood of a conversion or in other words a customer buying their products if the latter is targeted through a specific channel. Model understands the historical nuances and builds the internal model parameters so as to tell the team where they should focus on spending their marketing budget!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9658da15",
      "metadata": {
        "id": "9658da15"
      },
      "source": [
        "# **Possible Solutions**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c01152c4",
      "metadata": {
        "id": "c01152c4"
      },
      "source": [
        "## **Methods for Marketing Budget Optimization**\n",
        "\n",
        "\n",
        "There are various ways to build a model for marketing budget optimization, and the choice depends on factors such as the data available, the complexity of the system, and the computational resources available. Some approaches are:\n",
        "\n",
        "\n",
        "### **1. Rule-based systems:**\n",
        " In this approach, we typically look at budget proportion allocation based on predefined rules. Rule-based systems are easy to implement and interpret, but they may not be very accurate or adaptive. It makes sense to use them when you have a specific business mandate (example - ease of managing ads or segment of users you want to target).\n",
        "\n",
        "\n",
        "### **2. Statistical techniques:**\n",
        " Statistical techniques such as correlation, probability are very helpful to find the righ budget mix for each marketing channel. In this approach, we use historical data to come up with priors and correlations to do budget allocation.\n",
        "\n",
        "\n",
        "### **3. Linear optimizatiom:**\n",
        "In this approach, we define the objective function(e.g. increase conversion, reduce total cost) and the constraints(e.g. we should not spend more than 1000$ on Instagram) to come up with a linear equation. The linear equation is usually solved using a solver such as Gurobi, Pulp, etc. for the right solution which gives the amount of money company needs to spend on each channel.\n",
        "\n",
        "\n",
        "### **4. Machine learning:**\n",
        "Machine learning systems combines strength of historical data and statistical techniques to explain the right channel for individual customers of the company. For example, a ML system can tell with a high confidence if a potentail customer will click on the ad or not."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b84de32",
      "metadata": {
        "id": "5b84de32"
      },
      "source": [
        "### **Assumptions**\n",
        "\n",
        "* We assume that <b>Interest Level</b> is our target variable which refers to the interest of a user for a lead id\n",
        "* We assume that whenever the target variable is NA or Not called, the corresponding lead id is not meaningful and hence dropped\n",
        "* If the model's prediction is very close to 1, it means that the user is very likely to engage with the lead id\n",
        "* Columns with a lot of null values are not meaningful and imputation also won't be helpful"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8d5147ac",
      "metadata": {
        "id": "8d5147ac"
      },
      "source": [
        "## **Approach**\n",
        "\n",
        "\n",
        "We are treating this problem as an supervised learning problem. So every data point will have a target variable for the model to learn the dependencies and predict on the unknown.\n",
        "\n",
        "\n",
        "In real life, this model would tell the business whether a user is likely to engage with the ad or not and that would in turn help the company to allocate budgets accordingly.\n",
        "\n",
        "\n",
        "Given our assumptions about the data, we will build a prediction model based on the historical data. Simplifying, here's the logic of what we'll build:\n",
        "\n",
        "\n",
        "1. We'll build a model to identify if a customer will be interested in the lead;\n",
        "2. We'll use various tree based model and compare their performance on interest prediction;\n",
        "3. We will then choose the most successful model to use in production;\n",
        "\n",
        "* Exploratory Data Analysis (EDA):\n",
        "  * Understand the features and their relationships with target variables\n",
        "  * Check for missing or invalid values and their imputation\n",
        "\n",
        "\n",
        "* Data Preprocessing:\n",
        "  * Encode the variables using label encoding\n",
        "  * Split the dataset into training and testing sets\n",
        "\n",
        "* Model Building and Testing:\n",
        "  * Random Forest\n",
        "  * Light Gradient Boosting\n",
        "  * Extreme Gradient Boosting\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5654955d",
      "metadata": {
        "id": "5654955d"
      },
      "source": [
        "**Supervised Machine Learning:**\n",
        "\n",
        "In supervised machine learning, the algorithm is trained on an labeled dataset with a predefined target variable. The goal is to identify patterns, relationships, and structures of the data with the target variable, such as logistic regression, decision tree or boosting trees"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bbe3927d",
      "metadata": {
        "id": "bbe3927d"
      },
      "source": [
        "# **Learning Outcomes**\n",
        "\n",
        "\n",
        "\n",
        "* Understanding the importance of data-driven decision-making in marketing budget allocation.\n",
        "\n",
        "* Leveraging machine learning algorithms to optimize marketing budget allocation and maximize revenue and ROI.\n",
        "\n",
        "* Identifying the significance of targeting the right users through the appropriate channels and assets for improved conversions.\n",
        "\n",
        "* Data preprocessing, encoding, and splitting into training and testing sets\n",
        "\n",
        "* Recognizing the impact of targeted marketing on customer acquisition costs and improving customer retention.\n",
        "\n",
        "* Understanding and Implementing Tree-based models like Random Forest, Light Gradient Boosting, and Extreme Gradient Boosting to effectively predict interest.\n",
        "\n",
        "* What are PR and AUC curves? And how do they help in chosing the right threshold?\n",
        "\n",
        "* Improving overall business performance by aligning marketing efforts with customer preferences and behavior.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b846ebce",
      "metadata": {
        "id": "b846ebce"
      },
      "source": [
        "## **Prerequisites**\n",
        "\n",
        "* Familiarity with Python programming language\n",
        "\n",
        "* Familiarity with Pandas, sklearn, numpy libraries in Python, along with concepts like loops, lists, arrays and dataframe\n",
        "\n",
        "* Basic knowledge of machine learning concepts such as supervised learning, tree models\n",
        "\n",
        "* Understanding of data preprocessing techniques such as handling missing values, outliers, and categorical variables\n",
        "\n",
        "* Knowledge of Jupyter Notebook or any other Python IDE.\n",
        "\n",
        "* Understanding metrics such as precision, recall, PR curve, AUC curve\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ex65xkSH9f_B",
      "metadata": {
        "id": "ex65xkSH9f_B"
      },
      "source": [
        "## **Execution Instructions**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "hPCBQfuj9m8e",
      "metadata": {
        "id": "hPCBQfuj9m8e"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here to view more</summary>\n",
        "\n",
        "\n",
        "Play\" button next to the cell.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "v3WuzXSN-qS_",
      "metadata": {
        "id": "v3WuzXSN-qS_"
      },
      "source": [
        "## **Important Libraries**\n",
        "\n",
        "* **pandas**: pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool built on top of the Python programming language. Refer to [documentation](https://pandas.pydata.org/) for more information.\n",
        "\n",
        "* **NumPy**: The fundamental package for scientific computing with Python. Fast and versatile, the NumPy vectorization, indexing, and broadcasting concepts are the de-facto standards of array computing today. NumPy offers comprehensive mathematical functions, random number generators, linear algebra routines, Fourier transforms, and more. Refer to [documentation](https://numpy.org/) for more information. pandas and NumPy are together used for most of the data analysis and manipulation in Python.\n",
        "\n",
        "* **Matplotlib**: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Refer to [documentation](https://matplotlib.org/) for more information.\n",
        "\n",
        "* **seaborn**: Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Refer to [documentation](https://seaborn.pydata.org/) for more information.\n",
        "\n",
        "* **scikit-learn**: Simple and efficient tools for predictive data analysis\n",
        "accessible to everybody and reusable in various contexts.\n",
        "It is built on NumPy, SciPy, and matplotlib to support machine learning in Python. Refer to [documentation](https://scikit-learn.org/stable/) for more information.\n",
        "\n",
        "* **Warnings**:The warnings library provides a way to handle warnings that are generated during program execution. Warnings are typically issued when there is a potential issue with code, but the code still runs without errors. The warnings module provides a way to catch these warnings and handle them in a way that is appropriate for the program. This can be especially useful when developing and debugging code, as warnings can help identify potential issues before they become errors.\n",
        "\n",
        "* **sys**:The sys library provides access to some system-specific parameters and functions. This library can be used to access system-level information, such as the command line arguments passed to the program, the version of the Python interpreter being used, and more.\n",
        "\n",
        "* **xgboost**: xgboost is an open-source machine learning library designed to be highly efficient, scalable, and portable. It is a gradient boosting algorithm that is used for supervised learning problems, including regression, classification, and ranking. Refer to [documentation](https://xgboost.readthedocs.io/en/stable/install.html) for more information.\n",
        "\n",
        "\n",
        "* **lightgbm**: LightGBM is a gradient boosting framework that uses tree based learning algorithms. Refer to [documentation](https://lightgbm.readthedocs.io/en/v3.3.5/) for more information."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c4fc1bfa",
      "metadata": {
        "id": "c4fc1bfa"
      },
      "source": [
        "# **Package Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qGeRqhOz_KVv",
      "metadata": {
        "id": "qGeRqhOz_KVv"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f812a6",
      "metadata": {
        "id": "43f812a6",
        "outputId": "57f10385-f711-4c92-dfe1-01c0930d7925"
      },
      "outputs": [],
      "source": [
        "# python version 3.8.10\n",
        "\n",
        "!pip install numpy==1.20.0\n",
        "!pip install pandas==1.2.4\n",
        "!pip install matplotlib==3.7.1\n",
        "!pip install scikit-learn==0.23.2\n",
        "!pip install xgboost==1.6.2\n",
        "!pip install lightgbm==3.3.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "b124c0ed",
      "metadata": {
        "id": "b124c0ed"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "# Suppress all warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3144b116",
      "metadata": {
        "id": "3144b116"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', 200)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "32e31f28",
      "metadata": {
        "id": "32e31f28"
      },
      "source": [
        "Its the maximum number of columns displayed when a frame is pretty-printed.\n",
        "By setting this limit we can see 200 columns at once without truncation.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7ba55589",
      "metadata": {
        "id": "7ba55589"
      },
      "source": [
        "# **The Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588443c9",
      "metadata": {
        "id": "588443c9"
      },
      "outputs": [],
      "source": [
        "csv_file_path = \"https://s3.amazonaws.com/projerenata.com/marketing-spend-optimization-machine-learning-python/materials/Marketing_Data.csv\"\n",
        "df = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44be0c30",
      "metadata": {
        "id": "44be0c30"
      },
      "source": [
        "Lets look at the first 10 records from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e0aadd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "65e0aadd",
        "outputId": "c8b3c95c-b659-48f7-aa19-5f89bf969f35",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "472db6f4",
      "metadata": {
        "id": "472db6f4"
      },
      "source": [
        "If we look at the dataframe, we notice that there is an ID column and there are multiple columns with NA values.\n",
        "\n",
        "What we will do here is:\n",
        "1. Look for other such columns which don't serve value;\n",
        "2. Treat columns having missing values either by imputation or by dropping them;"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "819361ed",
      "metadata": {
        "id": "819361ed"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9a87ff70",
      "metadata": {
        "id": "9a87ff70"
      },
      "source": [
        "## **Data Exploration**\n",
        "\n",
        "Data exploration is a critical step in the data analysis process, where you examine the dataset to gain a preliminary understanding of the data, detect patterns, and identify potential issues that may need further investigation. Data exploration is important because it helps to provide a solid foundation for subsequent data analysis tasks, hypothesis testing and data visualization.\n",
        "\n",
        "Data exploration is also important because it can help you to identify an appropriate approach for analyzing the data.\n",
        "\n",
        "Here are the various functions that help us explore and understand the data.\n",
        "\n",
        "* Shape: Shape is used to identify the dimensions of the dataset. It gives the number of rows and columns present in the dataset. Knowing the dimensions of the dataset is important to understand the amount of data available for analysis and to determine the feasibility of different methods of analysis.\n",
        "\n",
        "* Head: The head function is used to display the top five rows of the dataset. It helps us to understand the structure and organization of the dataset. This function gives an idea of what data is present in the dataset, what the column headers are, and how the data is organized.\n",
        "\n",
        "* Tail: The tail function is used to display the bottom five rows of the dataset. It provides the same information as the head function but for the bottom rows. The tail function is particularly useful when dealing with large datasets, as it can be time-consuming to scroll through all the rows.\n",
        "\n",
        "* Describe: The describe function provides a summary of the numerical columns in the dataset. It includes the count, mean, standard deviation, minimum, and maximum values, as well as the quartiles. It helps to understand the distribution of the data, the presence of any outliers, and potential issues that can affect the model's accuracy.\n",
        "\n",
        "* Isnull: The isnull function is used to identify missing values in the dataset. It returns a Boolean value for each cell, indicating whether it is null or not. This function is useful to identify the presence of missing data, which can be problematic for regression analysis.\n",
        "\n",
        "* Dropna: The dropna function is used to remove rows or columns with missing data. It is used to remove any observations or variables with missing data, which can lead to biased results in the regression analysis. The dropna function is used after identifying the missing data with the isnull function.\n",
        "\n",
        "* Columns: The .columns method is a built-in function that is used to display the column names of a pandas DataFrame or Series. It returns an array-like object that contains the names of the columns in the order in which they appear in the original DataFrame or Series. It can be used to obtain a quick overview of the variables in a dataset and their names."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44b12f3d",
      "metadata": {
        "id": "44b12f3d"
      },
      "source": [
        "### **What can we learn from the data?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e749623a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e749623a",
        "outputId": "96dc0ff5-8adc-4054-f86b-09ab5062751c"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f887b4d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f887b4d9",
        "outputId": "a5d6d6f0-6b00-4d14-f042-9f87d31c192b"
      },
      "outputs": [],
      "source": [
        "# Checking the names of the columns\n",
        "df.columns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "14e94664",
      "metadata": {
        "id": "14e94664"
      },
      "source": [
        "## **Data Dictionary**\n",
        "\n",
        "\n",
        "\n",
        "| Column name\t | Description|\n",
        "| ----- | ----- |\n",
        "| Lead Id|  Unique Identifier |\n",
        "| Lead Owner|  Internal sales person associated with the lead |\n",
        "| Interest Level|  What is lead's interest level? (entered manually) |\n",
        "| Lead created|  Lead creation date |\n",
        "| Lead Location(Auto)|  Automatically detected location |\n",
        "| Creation Source|  Creation source of the lead |\n",
        "| Next activity|  Date for Next Activity |\n",
        "| What do you do currently ?|  Current profile of lead |\n",
        "| What are you looking for in Product ?|  Specific requirement from product |\n",
        "| Website Source|  Website Source of the Lead |\n",
        "| Lead Last Update time|  Last update time for Lead |\n",
        "| Marketing Source|  Marketing Source of the Lead |\n",
        "| Lead Location(Manual)|  Manually entered lead location |\n",
        "| Demo Date|  Date for Demo |\n",
        "| Demo Status|  Status of demo booked with lead |\n",
        "| Closure date|  Lead closing date |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8beee630",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "8beee630",
        "outputId": "c4a967e6-ae2c-4fc6-80c8-0a02ca7b5da1"
      },
      "outputs": [],
      "source": [
        "# Check the Information of the Dataframe, number of unique values and frequency\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae9af3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ae9af3f",
        "outputId": "f4d9c519-0c1e-402b-b17a-79274b7442bf"
      },
      "outputs": [],
      "source": [
        "# Check the Information of the Dataframe, datatypes and non-null counts\n",
        "df.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3bbf6183",
      "metadata": {
        "id": "3bbf6183"
      },
      "source": [
        "**Observation:**\n",
        "* we can see some null values present in this data. We will treat them later\n",
        "* Lead created, Next activity, Lead Last Update time and Demo Date should be datetime datatype but it is object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb1538f",
      "metadata": {
        "id": "efb1538f"
      },
      "outputs": [],
      "source": [
        "df['Lead created'] = pd.to_datetime(df['Lead created'], format=\"%d-%m-%Y %H:%M\")\n",
        "df['Lead Last Update time'] = pd.to_datetime(df['Lead Last Update time'], format=\"%d-%m-%Y %H:%M\")\n",
        "df['Next activity'] = pd.to_datetime(df['Next activity'], format=\"%d-%m-%Y %H:%M\")\n",
        "df['Demo Date'] = pd.to_datetime(df['Demo Date'], format=\"%d-%m-%Y %H:%M\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a8753a62",
      "metadata": {
        "id": "a8753a62"
      },
      "source": [
        "Lets see how many different Lead Owners we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2badf7cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2badf7cb",
        "outputId": "30954b40-0274-49b1-df04-1e8ece7e61bd"
      },
      "outputs": [],
      "source": [
        "df['Lead Owner'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc9d26e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bc9d26e",
        "outputId": "2183ca10-314d-4fec-95f0-d95847cd958b"
      },
      "outputs": [],
      "source": [
        "df['Lead Owner'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "26ce5211",
      "metadata": {
        "id": "26ce5211"
      },
      "source": [
        "**Observation**\n",
        "* The data seems to be evenly distributed amongst lead owners"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee61b3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee61b3f",
        "outputId": "473b4dd3-757a-4193-ec62-74406dd68d7e"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca285ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aca285ea",
        "outputId": "de509c8c-f5ca-441b-8a3f-d8cc3487ba36"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "84e6acc6",
      "metadata": {
        "id": "84e6acc6"
      },
      "source": [
        "**Observation**\n",
        "* We see that some of the interest levels are similar semantically\n",
        "* Since interest level is our target variable, it seems to be nicely distributed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7347fa30",
      "metadata": {
        "id": "7347fa30"
      },
      "source": [
        "**Points to ponder upon**\n",
        "* Should we formulate the problem as multi-class or binary classification problem?\n",
        "* In case, we want to do binary classification, how do we deal with many values in our target variable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afec9220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afec9220",
        "outputId": "7b3adacd-6b28-471c-e36f-aa9a5aab3bfc"
      },
      "outputs": [],
      "source": [
        "df['What do you do currently ?'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bffefd6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bffefd6d",
        "outputId": "cc7c2a5c-1b54-42ae-a700-d600d1a642c4"
      },
      "outputs": [],
      "source": [
        "df['What do you do currently ?'].unique().shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a894bb3d",
      "metadata": {
        "id": "a894bb3d"
      },
      "source": [
        "**Observation**\n",
        "* There are many unique values in the above column\n",
        "* If we process the string we can reduce these"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "93659f59",
      "metadata": {
        "id": "93659f59"
      },
      "source": [
        "\n",
        "* Do we need to focus on so many values and confuse the model?\n",
        "* What can we do to reduce these?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3bbf1f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3bbf1f6",
        "outputId": "71b5f333-1b0a-4929-ac9a-67c534ece622"
      },
      "outputs": [],
      "source": [
        "df['Creation Source'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88e1256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b88e1256",
        "outputId": "80d74904-a0eb-446c-c731-f6865d7485f0"
      },
      "outputs": [],
      "source": [
        "df['Creation Source'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "238271e7",
      "metadata": {
        "id": "238271e7"
      },
      "source": [
        "**Observation**\n",
        "* This feature looks well balanced in terms of unique values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d943605e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d943605e",
        "outputId": "3e9cc48d-7c51-4910-de6b-96b00d3c667a"
      },
      "outputs": [],
      "source": [
        "df['What are you looking for in Product ?'].unique().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88dfc533",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88dfc533",
        "outputId": "e694463f-88a2-40de-987b-4824eed9f6a2"
      },
      "outputs": [],
      "source": [
        "df['What are you looking for in Product ?'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "27ccc7eb",
      "metadata": {
        "id": "27ccc7eb"
      },
      "source": [
        "**Observation**\n",
        "* This feature has many unqiue values and processing it will take a lot of time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7fdb11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a7fdb11",
        "outputId": "2e80d0e9-7fe9-41b4-f29e-0411f6270852"
      },
      "outputs": [],
      "source": [
        "df['Website Source'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43c6ffdb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43c6ffdb",
        "outputId": "e6b7dc64-c268-43f9-e804-ff330c8c131b"
      },
      "outputs": [],
      "source": [
        "df['Website Source'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "09318ca6",
      "metadata": {
        "id": "09318ca6"
      },
      "source": [
        "**Observation**\n",
        "* Column has very less variance in terms of frequency\n",
        "* Most of the values are concentrated around 1 or 2 enums"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ace526a6",
      "metadata": {
        "id": "ace526a6"
      },
      "source": [
        "\n",
        "* Almost no variance in data, can model learn something important?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0cc98be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0cc98be",
        "outputId": "2062fb74-3eec-4f80-b5e6-b884753e662d"
      },
      "outputs": [],
      "source": [
        "df['Marketing Source'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4ee2037d",
      "metadata": {
        "id": "4ee2037d"
      },
      "source": [
        "**Observation**\n",
        "* There is a long tail of values\n",
        "* The 1st half looks really interesting in terms of distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d9c2e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9d9c2e1",
        "outputId": "61374b16-4141-4a6a-e158-fed423163e02"
      },
      "outputs": [],
      "source": [
        "df['Demo Status'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9883eec4",
      "metadata": {
        "id": "9883eec4"
      },
      "source": [
        "**Observation**\n",
        "* The column is nicely dsitributed\n",
        "* Has very less unique values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "751f61e1",
      "metadata": {
        "id": "751f61e1"
      },
      "source": [
        "\n",
        "* If we use this column, are we doing a feature leak?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d203d71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d203d71",
        "outputId": "fc7711b9-4911-4a3c-a959-9f0996359463"
      },
      "outputs": [],
      "source": [
        "df['Lead Location(Manual)'].value_counts(normalize=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4d96028e",
      "metadata": {
        "id": "4d96028e"
      },
      "source": [
        "**Observation**\n",
        "* This feature again has a very long tail"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "63efdc26",
      "metadata": {
        "id": "63efdc26"
      },
      "source": [
        "**Think about it**\n",
        "* What if we just create 2 enums; India and Non India"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f74ed9ea",
      "metadata": {
        "id": "f74ed9ea"
      },
      "source": [
        "# **Data Processing & Feature engineering**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d274025f",
      "metadata": {
        "id": "d274025f"
      },
      "source": [
        "### **Data Preprocessing and Leakage**\n",
        "\n",
        "Data leakage is a situation where information from the test or prediction data is inadvertently used during the training process of a machine learning model. This can occur when information from the test or prediction data is leaked into the training data, and the model uses this information to improve its performance during the training process.\n",
        "\n",
        "Data leakage can occur during the preprocessing phase of machine learning when information from the test or prediction data is used to preprocess the training data, inadvertently leaking information from the test or prediction data into the training data.\n",
        "\n",
        "\n",
        "To avoid data leakage, it's important to perform the data preprocessing steps on the training data only, and then apply the same preprocessing steps to the test and prediction data separately. This ensures that the test and prediction data remain unseen by the model during the training process, and helps to prevent overfitting and improve the accuracy of the model.\n",
        "\n",
        "In the context of this problem, we will perform data preprocessing steps together for the sake of simplicity, which could potentially lead to data leakage. However, in real-world scenarios, it's important to treat the test and prediction data separately and apply the necessary preprocessing steps separately, based on the characteristics of the data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b5ae2ed8",
      "metadata": {
        "id": "b5ae2ed8"
      },
      "source": [
        "### **Missing Value Detection and Imputation**\n",
        "\n",
        "Real world datasets are never friendly to data scientists. They always pose great challenges to those who are dealing with them due to many different reasons and one of them is “missing values”\n",
        "\n",
        "Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "51504bc4",
      "metadata": {
        "id": "51504bc4"
      },
      "source": [
        "We previously saw there are some missing values in the data. Lets have a look into that now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb1fc58",
      "metadata": {
        "id": "2fb1fc58"
      },
      "outputs": [],
      "source": [
        "# Lead Owner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b49383",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34b49383",
        "outputId": "f04358ae-742a-40ba-d100-2ac6d523f853"
      },
      "outputs": [],
      "source": [
        "df['Lead Owner'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c08c2d",
      "metadata": {
        "id": "f8c08c2d"
      },
      "outputs": [],
      "source": [
        "# Interest Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf04369",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbf04369",
        "outputId": "42528089-6070-4412-e69a-9be9834e41fd"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'].isna().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a751c7cd",
      "metadata": {
        "id": "a751c7cd"
      },
      "source": [
        "Since target variable has missing values, we will drop such rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee1e68d",
      "metadata": {
        "id": "1ee1e68d"
      },
      "outputs": [],
      "source": [
        "df = df[df['Interest Level'].notna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae56795",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ae56795",
        "outputId": "40b846ae-570f-4b29-994c-62e5884c4fe5"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0ba9e67a",
      "metadata": {
        "id": "0ba9e67a"
      },
      "source": [
        "With this mean, we will fill the NaN values."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ac69be7f",
      "metadata": {
        "id": "ac69be7f"
      },
      "source": [
        "##### Now we will handle our target variable\n",
        "\n",
        "Since there are multiple values in target variable and we want to formulate our problem as a binary classification problem, we will do the following assignments"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e4da659a",
      "metadata": {
        "id": "e4da659a"
      },
      "source": [
        "**Label assignment:**\n",
        "* Slightly Interested = 1\n",
        "* Not Interested=0\n",
        "* No Answer=0\n",
        "* Fairly Interested=1\n",
        "* Very Interested=1\n",
        "\n",
        "* we will drop rows where value is Not called, Closed or Invalid Number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222c5cc0",
      "metadata": {
        "id": "222c5cc0"
      },
      "outputs": [],
      "source": [
        "df = df[~df['Interest Level'].isin([\"Not called\", \"Closed\", \"Invalid Number\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd536f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cd536f8",
        "outputId": "329b75ac-6383-406e-ab00-41d33a1ce798"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9bea5c",
      "metadata": {
        "id": "4a9bea5c"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'] = df['Interest Level'].apply(lambda x: 1 if x in [\"Slightly Interested\", \"Fairly Interested\", \"Very Interested\"] else 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0a06e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf0a06e1",
        "outputId": "f882d24b-1710-402c-9d6f-e33378127fa8"
      },
      "outputs": [],
      "source": [
        "df['Interest Level'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7d6d1c15",
      "metadata": {
        "id": "7d6d1c15"
      },
      "source": [
        "#### Drop not imporant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12479545",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12479545",
        "outputId": "71c41024-aca3-4cbc-c302-877ef76ce2c2"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61c7bce",
      "metadata": {
        "id": "a61c7bce"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"Lead Id\", \"Lead Location(Auto)\", \"Next activity\", \"What are you looking for in Product ?\",\n",
        "              \"Lead Last Update time\", \"Lead Location(Manual)\", \"Demo Date\", \"Demo Status\", \"Closure date\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "443b25ed",
      "metadata": {
        "id": "443b25ed"
      },
      "source": [
        "#### Lead creation time\n",
        "\n",
        "- We will create 2 features here from our lead creation time column\n",
        "    1. hour of day\n",
        "    2. day of week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d264b4d7",
      "metadata": {
        "id": "d264b4d7"
      },
      "outputs": [],
      "source": [
        "df['hour_of_day'] = df['Lead created'].dt.hour\n",
        "df['day_of_week'] = df['Lead created'].dt.weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c036477",
      "metadata": {
        "id": "4c036477"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"Lead created\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19f4a98a",
      "metadata": {
        "id": "19f4a98a"
      },
      "source": [
        "Now lead created column is not useful and we drop it"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b2f10f0e",
      "metadata": {
        "id": "b2f10f0e"
      },
      "source": [
        "#### Creation source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2c683c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c2c683c",
        "outputId": "ea517c29-c100-4f8a-dec6-3d3ec53dafeb"
      },
      "outputs": [],
      "source": [
        "df['Creation Source'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a78803",
      "metadata": {
        "id": "e8a78803"
      },
      "outputs": [],
      "source": [
        "from pandas import factorize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3bd9c05",
      "metadata": {
        "id": "b3bd9c05"
      },
      "outputs": [],
      "source": [
        "labels, categories = factorize(df[\"Creation Source\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077bad84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "077bad84",
        "outputId": "65d7ca1f-dfb1-4643-bee5-0703f77dbcd6"
      },
      "outputs": [],
      "source": [
        "df[\"labels\"] = labels\n",
        "abs(df[\"Interest Level\"].corr(df[\"labels\"]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "106e4d2e",
      "metadata": {
        "id": "106e4d2e"
      },
      "source": [
        "There is a positive correlation with the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a49fa2",
      "metadata": {
        "id": "66a49fa2"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"labels\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "def6f681",
      "metadata": {
        "id": "def6f681"
      },
      "source": [
        "#### What do you do currently?\n",
        "\n",
        "* student = 1\n",
        "* others = 0\n",
        "\n",
        "As we saw earlier, this feature has a large number of values of which students are a dominating part.\n",
        "\n",
        "We will binarize this column into students and non-students"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "757bc0fd",
      "metadata": {
        "id": "757bc0fd"
      },
      "source": [
        "**Facts**\n",
        "\n",
        "<i>Binarization</i> is the process of dividing data into two groups and assigning one out. of two values to all the members of the same group. This is usually accomplished. by defining a threshold t and assigning the value 0 to all the data points below. the threshold and 1 to those above it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeaeba29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeaeba29",
        "outputId": "4724d979-ed61-4cce-9145-134eba541192"
      },
      "outputs": [],
      "source": [
        "df['What do you do currently ?'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d83d72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06d83d72",
        "outputId": "ca8a56a4-f2cf-4f8f-890e-adce00346cee"
      },
      "outputs": [],
      "source": [
        "df['What do you do currently ?'].value_counts(normalize=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d78f5f5",
      "metadata": {
        "id": "8d78f5f5"
      },
      "outputs": [],
      "source": [
        "df['What do you do currently ?'] = df['What do you do currently ?'].apply(lambda x: 1 if 'student' in str(x).strip().lower() else 0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3f4cde99",
      "metadata": {
        "id": "3f4cde99"
      },
      "source": [
        "#### Website Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5162ba9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5162ba9",
        "outputId": "58ac2805-911e-481a-e625-a961b474e64d"
      },
      "outputs": [],
      "source": [
        "df['Website Source'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cc8280",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2cc8280",
        "outputId": "5b28dc85-dac7-46dd-b878-4d9ea02d6a51"
      },
      "outputs": [],
      "source": [
        "df['Website Source'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a299a270",
      "metadata": {
        "id": "a299a270"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"Website Source\"], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5eeab9bf",
      "metadata": {
        "id": "5eeab9bf"
      },
      "source": [
        "Dropping the <b>Website Source</b> column as there is not enough variance"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "787507aa",
      "metadata": {
        "id": "787507aa"
      },
      "source": [
        "#### Marketing Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a6c1f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58a6c1f6",
        "outputId": "0b995910-6c7f-4afd-c1f7-96585d1a7807"
      },
      "outputs": [],
      "source": [
        "df['Marketing Source'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ac9b1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13ac9b1f",
        "outputId": "16b4af38-da77-4b14-d3b4-80162ec68d07"
      },
      "outputs": [],
      "source": [
        "df['Marketing Source'].isna().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ebe26a5c",
      "metadata": {
        "id": "ebe26a5c"
      },
      "source": [
        "Marketing Source has a large number of missing value and it will be noisy if we do an imputation here.\n",
        "\n",
        "Rather, let's create a new value <b>Unknown</b> which will be substituted for NA values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e82302",
      "metadata": {
        "id": "82e82302"
      },
      "outputs": [],
      "source": [
        "df['Marketing Source'].fillna(\"Unknown\", inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5f0e27e8",
      "metadata": {
        "id": "5f0e27e8"
      },
      "source": [
        "PS: Imputation with Unknown led to improvements that dropping these rows"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "20cabce1",
      "metadata": {
        "id": "20cabce1"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0e8dc34f",
      "metadata": {
        "id": "0e8dc34f"
      },
      "source": [
        "**Transforming Categorical Variables**\n",
        "\n",
        "Transforming variables is an important step in the data preprocessing pipeline of machine learning, as it helps to convert the data into a format that is suitable for analysis and modeling. There are several ways to transform variables, depending on the type and nature of the data.\n",
        "\n",
        "Categorical variables, for example, are variables that take on discrete values from a finite set of categories, such as colors, gender, or occupation. One common way to transform categorical variables is through one-hot encoding. One-hot encoding involves creating a new binary variable for each category in the original variable, where the value is 1 if the observation belongs to that category and 0 otherwise. This approach is useful when the categories have no natural order or ranking.\n",
        "\n",
        "Another way to transform categorical variables is through label encoding. Label encoding involves assigning a unique integer value to each category in the variable. This approach is useful when the categories have a natural order or ranking, such as low, medium, and high.\n",
        "Transforming categorical features into numerical labels:\n",
        "\n",
        "**Note:** We are NOT using dummies here to minimize the explosion of columns because of the distance methods we are using.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bad061b",
      "metadata": {
        "id": "0bad061b"
      },
      "outputs": [],
      "source": [
        "label_encoder1 = preprocessing.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0580d1e",
      "metadata": {
        "id": "c0580d1e"
      },
      "outputs": [],
      "source": [
        "df['Marketing Source']= label_encoder1.fit_transform(df['Marketing Source'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ca91f8",
      "metadata": {
        "id": "c8ca91f8"
      },
      "outputs": [],
      "source": [
        "label_encoder2 = preprocessing.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2ddc24",
      "metadata": {
        "id": "ee2ddc24"
      },
      "outputs": [],
      "source": [
        "df['Lead Owner']= label_encoder2.fit_transform(df['Lead Owner'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c191d4c",
      "metadata": {
        "id": "5c191d4c"
      },
      "outputs": [],
      "source": [
        "label_encoder3 = preprocessing.LabelEncoder()\n",
        "df['Creation Source']= label_encoder3.fit_transform(df['Creation Source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b06bbf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1b06bbf2",
        "outputId": "10026ea1-1f28-4d99-82bf-74c5f7ffba4d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0ee8a1c7",
      "metadata": {
        "id": "0ee8a1c7"
      },
      "source": [
        "We transformed 3 columns using label encoding\n",
        "\n",
        "Remember one thing, you should always use the same label encoding variable for test dataset. Since here we are handling train/test together, we are not worrying about it"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5c348cd6",
      "metadata": {
        "id": "5c348cd6"
      },
      "source": [
        "# **Model Building and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb206b5",
      "metadata": {
        "id": "cbb206b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_curve, roc_curve, plot_roc_curve, plot_precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ab5cb15e",
      "metadata": {
        "id": "ab5cb15e"
      },
      "source": [
        "Identify the right features for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f22e051",
      "metadata": {
        "id": "5f22e051"
      },
      "outputs": [],
      "source": [
        "X = df[[\"Lead Owner\", \"What do you do currently ?\", \"Marketing Source\", \"Creation Source\", \"hour_of_day\", \"day_of_week\"]]\n",
        "y = df[\"Interest Level\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d16b1e64",
      "metadata": {
        "id": "d16b1e64"
      },
      "source": [
        "**Splitting the dataset into a training and production dataset:**\n",
        "\n",
        "- Training: Part of data used for training our supervised models\n",
        "- Test: Part of the dataset used for testing our models performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ee0e32",
      "metadata": {
        "id": "e5ee0e32"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be779820",
      "metadata": {
        "id": "be779820",
        "outputId": "ca9c76cf-1e5c-436c-c969-418214caff0b"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "86132c80",
      "metadata": {
        "id": "86132c80"
      },
      "source": [
        "We finally have prepared model ready data.\n",
        "\n",
        "Lets look into model building now."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1dd5fa7c",
      "metadata": {
        "id": "1dd5fa7c"
      },
      "source": [
        "## **Supervised learning**\n",
        "\n",
        "\n",
        "\n",
        "Supervised learning uses a training set to teach models to yield the desired output. This training dataset includes inputs and correct outputs, which allow the model to learn over time. The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized.\n",
        "\n",
        "Supervised learning can be separated into two types of problems when data mining—classification and regression:\n",
        "\n",
        "1. Classification uses an algorithm to accurately assign test data into specific categories. It recognizes specific entities within the dataset and attempts to draw some conclusions on how those entities should be labeled or defined. Common classification algorithms are linear classifiers, support vector machines (SVM), decision trees, k-nearest neighbor, and random forest, which are described in more detail below.\n",
        "\n",
        "\n",
        "2. Regression is used to understand the relationship between dependent and independent variables. It is commonly used to make projections, such as for sales revenue for a given business. Linear regression, logistical regression, and polynomial regression are popular regression algorithms.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "da881f41",
      "metadata": {
        "id": "da881f41"
      },
      "source": [
        "## **Decision Tree**\n",
        "\n",
        "\n",
        "\n",
        "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
        "\n",
        "\n",
        "Decision tree learning employs a divide and conquer strategy by conducting a greedy search to identify the optimal split points within a tree. This process of splitting is then repeated in a top-down, recursive manner until all, or the majority of records have been classified under specific class labels. Whether or not all data points are classified as homogenous sets is largely dependent on the complexity of the decision tree. Smaller trees are more easily able to attain pure leaf nodes—i.e. data points in a single class.\n",
        "\n",
        "However, as a tree grows in size, it becomes increasingly difficult to maintain this purity, and it usually results in too little data falling within a given subtree. When this occurs, it is known as data fragmentation, and it can often lead to overfitting. As a result, decision trees have preference for small trees, which is consistent with the principle of parsimony in Occam’s Razor; that is, “entities should not be multiplied beyond necessity.” Said differently, decision trees should add complexity only if necessary, as the simplest explanation is often the best. To reduce complexity and prevent overfitting, pruning is usually employed; this is a process, which removes branches that split on features with low importance. The model’s fit can then be evaluated through the process of cross-validation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6b6d0234",
      "metadata": {},
      "source": [
        "### **Bagging**\n",
        "\n",
        "\n",
        "\n",
        "Bagging is an ensemble learning technique that aims to decrease the variance of a single estimator by combining the predictions from multiple learners. The basic idea behind bagging is to generate multiple versions of the training dataset through random sampling with replacement, and then train a separate classifier for each sampled dataset. The predictions from these individual classifiers are then combined using averaging or voting to obtain a final prediction.\n",
        "\n",
        "**Algorithm:**\n",
        "\n",
        "Suppose we have a training set D of size n, and we want to train a classifier using bagging. Here are the steps involved:\n",
        "\n",
        "* Create k different bootstrap samples from D, each of size n.\n",
        "* Train a classifier on each bootstrap sample.\n",
        "* When making predictions on a new data point, take the average or majority vote of the predictions from each of the k classifiers.\n",
        "\n",
        "\n",
        "**Mathematical Explanation:**\n",
        "\n",
        "Suppose we have a binary classification problem with classes -1 and 1. Let's also assume that we have a training set D of size n, and we want to train a decision tree classifier using bagging.\n",
        "\n",
        "**Bootstrap Sample**: For each of the k classifiers, we create a bootstrap sample of size n by sampling with replacement from D. This means that each bootstrap sample may contain duplicates of some instances and may also miss some instances from the original dataset. Let's denote the i-th bootstrap sample as D_i.\n",
        "\n",
        "**Train a Classifier**: We train a decision tree classifier T_i on each bootstrap sample D_i. This gives us k classifiers T_1, T_2, ..., T_k.\n",
        "\n",
        "**Combine Predictions**: To make a prediction on a new data point x, we take the majority vote of the predictions from each of the k classifiers.\n",
        "\n",
        "The idea behind bagging is that the variance of the prediction error decreases as k increases. This is because each classifier has a chance to explore a different part of the feature space due to the random sampling with replacement, and the final prediction is a combination of these diverse classifiers.\n",
        "\n",
        "\n",
        "\n",
        "### **Boosting**\n",
        "\n",
        "Boosting is a machine learning algorithm that works by combining several weak models (also known as base learners) into a strong model. The goal of boosting is to reduce the bias and variance of the base learners by iteratively adding new models to the ensemble that focus on correcting the errors made by the previous models. In other words, the boosting algorithm tries to learn from the mistakes of the previous models and improve the overall accuracy of the ensemble.\n",
        "\n",
        "Boosting works by assigning higher weights to the data points that the previous models misclassified, and lower weights to the ones that were classified correctly. This ensures that the new model focuses more on the difficult data points that the previous models struggled with, and less on the ones that were already well-classified. As a result, the new model is more specialized and can improve the accuracy of the ensemble.\n",
        "\n",
        "There are several types of boosting algorithms, including AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). Each of these algorithms has its own approach to assigning weights to the data points and building the new models, but they all share the fundamental idea of iteratively improving the accuracy of the ensemble by combining weak models into a strong one. Boosting is a powerful algorithm that has been shown to achieve state-of-the-art results in many machine learning tasks, such as image classification, natural language processing, and recommender systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Difference between Bagging and Boosting**\n",
        "\n",
        "\n",
        "It's important to remember that boosting is a generic method, not a specific model, in order to comprehend it. Boosting involves specifying a weak model, such as regression or decision trees, and then improving it. In Ensemble Learning, the primary difference between Bagging and Boosting is that in bagging, weak learners are trained in simultaneously, but in boosting, they are trained sequentially. This means that each new model iteration increases the weights of the prior model's misclassified data. This redistribution of weights aids the algorithm in determining which parameters it should focus on in order to increase its performance.\n",
        "\n",
        "Both the Ensemble techniques are used in a different way as well.  Bagging methods, for example, are often used on poor learners who have large variance and low bias such as decision trees because they tend to overfit, whereas boosting methods are employed when there is low variance and high bias. While bagging can help prevent overfitting, boosting methods are more vulnerable to it because of a simple fact they continue to build on weak learners and continue to minimise error. This can lead to overfitting on the training data but specifying a decent number of models to be generated or hyperparameter tuning,  regularization can help in this case, if overfitting encountered.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6438753e",
      "metadata": {},
      "source": [
        "### **Random Forest**\n",
        "\n",
        "\n",
        "Another way that decision trees can maintain their accuracy is by forming an ensemble via a random forest algorithm; this classifier predicts more accurate results, particularly when the individual trees are uncorrelated with each other.\n",
        "\n",
        "Random Forest is an ensemble learning algorithm that builds a large number of decision trees and combines them to make a final prediction. It is a type of bagging method, where multiple decision trees are trained on random subsets of the training data and features. The algorithm then averages the predictions of these individual trees to produce a final prediction. Random Forest is particularly useful for handling high-dimensional data and for avoiding overfitting.\n",
        "\n",
        "**Algorithm of Random Forest**\n",
        "\n",
        "The algorithm of Random Forest can be summarized in the following steps:\n",
        "\n",
        "* Start by randomly selecting a subset of the training data, with replacement. This subset is called the bootstrap sample.\n",
        "\n",
        "* Next, randomly select a subset of features from the full feature set.\n",
        "\n",
        "* Build a decision tree using the bootstrap sample and the selected subset of features. At each node of the tree, select the best feature and split the data based on the selected feature.\n",
        "\n",
        "* Repeat steps 1-3 to build multiple trees.\n",
        "\n",
        "* Finally, combine the predictions of all trees to make a final prediction. For classification, this is usually done by taking a majority vote of the predicted classes. For regression, this is usually done by taking the average of the predicted values.\n",
        "\n",
        "\n",
        "**Mathematics Behind Random Forest**\n",
        "\n",
        "The mathematics behind Random Forest involves the use of decision trees and the bootstrap sampling technique. Decision trees are constructed using a recursive binary partitioning algorithm that splits the data based on the values of the selected features. At each node, the algorithm chooses the feature and the split point that maximizes the information gain. Information gain measures the reduction in entropy or impurity of the target variable after the split. The goal is to minimize the impurity of the subsets after each split.\n",
        "\n",
        "Bootstrap sampling is a statistical technique that involves randomly sampling the data with replacement to create multiple subsets. These subsets are used to train individual decision trees. By using bootstrap samples, the algorithm can generate multiple versions of the same dataset with slightly different distributions. This introduces randomness into the training process, which helps to reduce overfitting.\n",
        "\n",
        "\n",
        "\n",
        "**Difference between Bagging and Random Forest**\n",
        "\n",
        "Bagging and Random Forest are both ensemble learning algorithms that involve training multiple models on random subsets of the data. The main difference between the two is the way the individual models are trained.\n",
        "\n",
        "Bagging involves training multiple models using the bootstrap sampling technique, but each model uses the same set of features. This can lead to correlated predictions, which reduces the variance but not necessarily the bias of the model.\n",
        "\n",
        "Random Forest, on the other hand, involves training multiple models using the bootstrap sampling technique, but each model uses a randomly selected subset of features. This introduces additional randomness into the model and helps to reduce the correlation between individual predictions. Random Forest can achieve better performance than Bagging, especially when dealing with high-dimensional data or noisy features. In simpler terms it uses subsets of observations as well as features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9b4cb623",
      "metadata": {
        "id": "9b4cb623"
      },
      "source": [
        "## **Gradient Boosting Trees**\n",
        "\n",
        "Gradient boosting is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees.\n",
        "\n",
        "When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function.\n",
        "\n",
        "Few examples of gradient boosting trees are Xgboost, LightGBM, etc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a5e21ad",
      "metadata": {
        "id": "7a5e21ad"
      },
      "source": [
        "## **Bagging vs Boosting**\n",
        "\n",
        "\n",
        "1. Bagging: It is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average.\n",
        "\n",
        "![image.png](https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png)\n",
        "\n",
        "\n",
        "2. Boosting: It is also a homogeneous weak learners’ model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm.\n",
        "\n",
        "![image.png](https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png)\n",
        "\n",
        "Credit: geeksforgeeks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876217ff",
      "metadata": {
        "id": "876217ff"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=300)\n",
        "xgb = XGBClassifier(n_estimators=300, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3)\n",
        "lgb = LGBMClassifier(n_estimators=300)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "21a59533",
      "metadata": {
        "id": "21a59533"
      },
      "source": [
        "#### Training multiple models together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44028c29",
      "metadata": {
        "id": "44028c29",
        "outputId": "60050c36-4e72-4eff-d3bc-08a4a258ea27"
      },
      "outputs": [],
      "source": [
        "rf.fit(X_train, y_train)\n",
        "xgb.fit(X_train, y_train)\n",
        "lgb.fit(X_train, y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "311e02e5",
      "metadata": {
        "id": "311e02e5"
      },
      "source": [
        "All models are trained very qucikly here.\n",
        "\n",
        "Scitkit-learn provides an additional parameter n_jobs=-1 which parallelize some of the models using cpu threads"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d630bb00",
      "metadata": {
        "id": "d630bb00"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e023f4dd",
      "metadata": {},
      "source": [
        "### **Classification Evaluation Metrics**\n",
        "\n",
        "Classification evaluation metrics are used to evaluate the performance of a machine learning model that is trained for classification tasks. Some of the commonly used classification evaluation metrics are F1 score, recall score, confusion matrix, and ROC AUC score. Here's an overview of each of these metrics:\n",
        "\n",
        "**F1 score**: The F1 score is a metric that combines the precision and recall of a model into a single value. It is calculated as the harmonic mean of precision and recall, and is expressed as a value between 0 and 1, where 1 indicates perfect precision and recall.\n",
        "F1 score is the harmonic mean of precision and recall. It is calculated as follows:\n",
        "$$ F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} $$\n",
        "where precision is the number of true positives divided by the sum of true positives and false positives, and recall is the number of true positives divided by the sum of true positives and false negatives.\n",
        "\n",
        "**Recall**: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
        "Recall score (also known as sensitivity) is the number of true positives divided by the sum of true positives and false negatives. It is given by the following formula:\n",
        "$$ Recall = \\frac{TP}{TP + FN} $$\n",
        "\n",
        "**Precision**: Precision is another important classification evaluation metric, which is defined as the ratio of true positives to the total predicted positives. It measures the accuracy of positive predictions made by the classifier, i.e., the proportion of positive identifications that were actually correct.\n",
        "The formula for precision is:\n",
        "$$ precision = \\frac{true\\ positive}{true\\ positive + false\\ positive} $$\n",
        "where true positive refers to the cases where the model correctly predicted the positive class, and false positive refers to the cases where the model incorrectly predicted the positive class.\n",
        "Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
        "\n",
        "**Confusion Matrix**:\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model. It compares the predicted labels with the true labels and counts the number of true positives, false positives, true negatives, and false negatives. Here is an example of a confusion matrix:\n",
        "\n",
        "|          | Actual Positive | Actual Negative |\n",
        "|----------|----------------|----------------|\n",
        "| Predicted Positive | True Positive (TP) | False Positive (FP) |\n",
        "| Predicted Negative | False Negative (FN) | True Negative (TN) |\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "\n",
        "**ROC AUC Score**:\n",
        "ROC AUC (Receiver Operating Characteristic Area Under the Curve) score is a measure of how well a classifier is able to distinguish between positive and negative classes. It is calculated as the area under the ROC curve. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. TPR is the number of true positives divided by the sum of true positives and false negatives, and FPR is the number of false positives divided by the sum of false positives and true negatives.\n",
        "$$ ROC\\ AUC\\ Score = \\int_0^1 TPR(FPR^{-1}(t)) dt $$\n",
        "where $FPR^{-1}$ is the inverse of the FPR function.\n",
        "\n",
        "**When to use which**:\n",
        "\n",
        "The choice of evaluation metric depends on the specific requirements of the business problem. Here are some general guidelines:\n",
        "\n",
        "* F1 score: Use the F1 score when the class distribution is imbalanced, and when both precision and recall are equally important.\n",
        "\n",
        "* Recall score: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
        "\n",
        "* Precision: Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
        "\n",
        "* Confusion matrix: The confusion matrix is a versatile tool that can be used to visualize the performance of a model across different classes. It can be useful for identifying specific areas of the model that need improvement.\n",
        "\n",
        "* ROC AUC score: Use the ROC AUC score when the ability to distinguish between positive and negative classes is important. For example, in a credit scoring problem, the ability to distinguish between good and bad credit risks is crucial.\n",
        "\n",
        "Importance with respect to the business problem:\n",
        "\n",
        "The importance of each evaluation metric varies depending on the business problem. For example, in a spam detection problem, precision may be more important than recall, since false positives (i.e., classifying a non-spam email as spam) may annoy users, while false negatives (i.e., missing a spam email) may not be as harmful. On the other hand, in a disease diagnosis problem, recall may be more important than precision, since missing a positive case (i.e., a false negative) could have serious consequences. Therefore, it is important to choose the evaluation metric that is most relevant to the specific business problem at hand.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ac8f43c8",
      "metadata": {
        "id": "ac8f43c8"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "![image.png](https://blog.paperspace.com/content/images/2020/09/Fig01.jpg)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "efd4f662",
      "metadata": {
        "id": "efd4f662"
      },
      "source": [
        "1. Accuracy\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Accuracy} = \\frac{True Positive + True Negative}{True Positive + False Positive + True Negative + False Negative}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "2. Precision\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Precision} = \\frac{True Positive}{True Positive + False Positive}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "3. Recall\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Recall} = \\frac{True Positive}{True Positive + False Negative}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "4. F1-score\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{F1-score} = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
        "\\end{equation}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dda343ec",
      "metadata": {},
      "source": [
        "Precision-Recall (PR) curve and Area Under the Curve (AUC) curve are evaluation metrics commonly used in binary classification problems to assess the performance of a model and determine an appropriate threshold for decision making.\n",
        "\n",
        "The Precision-Recall (PR) curve is a graphical representation of the trade-off between precision and recall for different classification thresholds. Precision is the ratio of true positive predictions to the total number of positive predictions, while recall (also known as sensitivity or true positive rate) is the ratio of true positive predictions to the total number of actual positive instances in the data. The PR curve plots precision on the y-axis and recall on the x-axis, with each point on the curve representing a different classification threshold. A higher precision and recall indicate better model performance.\n",
        "\n",
        "The Area Under the Curve (AUC) is a single scalar value that summarizes the PR curve. It measures the overall performance of the model across all possible classification thresholds. The AUC value ranges from 0 to 1, where a higher value indicates better model performance. An AUC of 1 represents a perfect model that achieves maximum precision and recall across all thresholds.\n",
        "\n",
        "Choosing the right threshold depends on the specific requirements of your problem. The PR curve can help you visualize the precision-recall trade-off at different thresholds. If your problem prioritizes precision (minimizing false positives), you may want to choose a threshold that maximizes precision while maintaining a reasonable level of recall. On the other hand, if recall is more important (minimizing false negatives), you would choose a threshold that maximizes recall while still maintaining an acceptable level of precision.\n",
        "\n",
        "The selection of the threshold ultimately depends on the cost or impact of false positives and false negatives in your specific problem domain. By analyzing the PR curve and considering the specific requirements and trade-offs of your problem, you can make an informed decision about the threshold that best balances precision and recall for your particular use case.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d82e76a",
      "metadata": {
        "id": "7d82e76a"
      },
      "outputs": [],
      "source": [
        "def get_evaluation_metrics(model_name, model, pred, actual):\n",
        "    print(\"Accuracy of %s: \" % model_name, accuracy_score(pred, actual))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a29ac28",
      "metadata": {
        "id": "4a29ac28",
        "outputId": "8fe9aff2-192b-4e67-d349-b62f15ee6f16"
      },
      "outputs": [],
      "source": [
        "get_evaluation_metrics(\"Random Forest\", rf, rf.predict(X_test), y_test)\n",
        "get_evaluation_metrics(\"XGBoost\", xgb, xgb.predict(X_test), y_test)\n",
        "get_evaluation_metrics(\"Light GBM\", lgb, lgb.predict(X_test), y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09274f1e",
      "metadata": {
        "id": "09274f1e",
        "outputId": "1456a65a-745b-45c9-94e7-9a843acd3a5e"
      },
      "outputs": [],
      "source": [
        "plot_precision_recall_curve(rf, X_test, y_test)\n",
        "plot_roc_curve(rf, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6d2332",
      "metadata": {
        "id": "8d6d2332",
        "outputId": "0d418771-e2dd-43dc-a786-6786b43e1f09"
      },
      "outputs": [],
      "source": [
        "plot_precision_recall_curve(xgb, X_test, y_test)\n",
        "plot_roc_curve(xgb, X_test, y_test)\n",
        "checkpoint(\"fcMar1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de17ac9",
      "metadata": {
        "id": "6de17ac9",
        "outputId": "edf9b4eb-44ee-4127-b7f0-fac7f03f6d03"
      },
      "outputs": [],
      "source": [
        "plot_precision_recall_curve(lgb, X_test, y_test)\n",
        "plot_roc_curve(lgb, X_test, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "131ca905",
      "metadata": {
        "id": "131ca905"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In this project we used a bunch of supervised models to predict if the customer would be interested in a lead.\n",
        "\n",
        "The problem could have been formulated as a multi-class classification problem but we instead formulated this as a binary classification problem and the confidence on the predictions would enable the stakeholders to chase the lead.\n",
        "\n",
        "\n",
        "A successful data science project requires a clear understanding of the business problem and the data available, as well as the ability to select and apply appropriate data preprocessing techniques, feature engineering methods, and machine learning algorithms. It is also important to assess and optimize the performance of the model and communicate the results effectively to stakeholders.\n",
        "\n",
        "After looking at the PR and ROC curves above, we can conclude that <b>LightGBM</b> is giving us the best possible results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
